{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic Dataset prediction using TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lishat/Neural-Networks-on-Titanic-Dataset/blob/master/Titanic_Dataset_prediction_using_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmtofghd72_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jumKwnAD8Yhi",
        "colab_type": "code",
        "outputId": "dbe9d396-fa44-4b11-d49d-1b60a3ca4e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data = pd.read_csv(\"train.csv\")\n",
        "data = data.drop(columns=[\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"])\n",
        "data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].mean())\n",
        "data = pd.concat([data.drop(columns=[\"Sex\"]),pd.get_dummies(data[\"Sex\"], drop_first=True)], axis=1)\n",
        "data = pd.concat([data.drop(columns=[\"Embarked\"]),pd.get_dummies(data[\"Embarked\"], drop_first=True)], axis=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(data.drop(columns=[\"Survived\"]), data[\"Survived\"], test_size=0.25)\n",
        "y_train = (np.array(y_train)).reshape(668, 1)\n",
        "y_test = (np.array(y_test)).reshape(223, 1)\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(668, 8) (668, 1) (223, 8) (223, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQgIf2mC87EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ANN(x_train, x_test, y_train, y_test, hidden_units1 = 30, hidden_units2 = 15, learning_rate = 0.001, epochs = 100000):\n",
        "  x = tf.placeholder(tf.float32, [None, 8])\n",
        "  y = tf.placeholder(tf.float32, [None, 1])\n",
        "  initializer = tf.contrib.layers.xavier_initializer()\n",
        "  W1 = tf.Variable(initializer([8, hidden_units1]), name=\"W1\")\n",
        "  b1 = tf.Variable(initializer([hidden_units1]), name=\"b1\")\n",
        "  z1 = tf.matmul(x, W1) + b1\n",
        "  a1 = tf.nn.relu(z1)\n",
        "  W2 = tf.Variable(initializer([hidden_units1, hidden_units2]), name=\"W2\")\n",
        "  b2 = tf.Variable(initializer([hidden_units2]), name=\"b2\")\n",
        "  z2 = tf.matmul(a1, W2) + b2\n",
        "  a2 = tf.nn.relu(z2)\n",
        "  W3 = tf.Variable(initializer([hidden_units2, 1]), name=\"W3\")\n",
        "  b3 = tf.Variable(initializer([1]), name=\"b3\")\n",
        "  z3 = tf.matmul(a2, W3) + b3\n",
        "  a3 = tf.nn.sigmoid(z3)\n",
        "  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=z3, labels = y))\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(a3), y), tf.float32))\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for epoch in range(epochs):\n",
        "      _, c = sess.run([optimizer, loss], feed_dict={x:x_train, y:y_train})\n",
        "      if epoch%1000 == 0:\n",
        "        print(\"LOSS after\", epoch, \"th epoch:\", c)\n",
        "    train_accuracy = sess.run(accuracy, feed_dict={x:x_train, y:y_train})\n",
        "    test_accuracy = sess.run(accuracy, feed_dict={x:x_test, y:y_test})\n",
        "    print(\"Train Accuracy:\", train_accuracy*100)\n",
        "    print(\"Test Accuracy:\", test_accuracy*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpQTa21T-P5a",
        "colab_type": "code",
        "outputId": "3f86b042-212e-4272-8609-c6f8acd1eb7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "ANN(x_train, x_test, y_train, y_test, epochs = 10000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOSS after 0 th epoch: 2.5288172\n",
            "LOSS after 1000 th epoch: 0.40180144\n",
            "LOSS after 2000 th epoch: 0.35216215\n",
            "LOSS after 3000 th epoch: 0.31895703\n",
            "LOSS after 4000 th epoch: 0.28917077\n",
            "LOSS after 5000 th epoch: 0.26857778\n",
            "LOSS after 6000 th epoch: 0.257326\n",
            "LOSS after 7000 th epoch: 0.24952659\n",
            "LOSS after 8000 th epoch: 0.24462199\n",
            "LOSS after 9000 th epoch: 0.24109721\n",
            "Train Accuracy: 90.56886434555054\n",
            "Test Accuracy: 81.16592168807983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IlWJxfLAMdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}